\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{capt-of} 
\usepackage{cuted} 

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{Title: Leveraging K-Means Clustering for Movie Recommendation Systems: A Feature-Reduction Approach Using IMDb Tags}

\author{Christian Schmitz, Lu Knoblich}

% The paper headers
\markboth{MLWR Report, Christian Schmitz (11120775) and Lu Knoblich (11161417) }%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

\maketitle
\input{CH1_Introduction}
\input{CH2_Data}
\input{CH3_k-means}
\input{CH4_Implementation}
\input{CH5_Conclusion}



% \chapter{Second}
% This is my Second Appendix .

\clearpage
\begin{thebibliography}{1}

\bibitem{ref_MovieLens}
https://grouplens.org/datasets/movielens/ (Accessed: 14.05.23)

\bibitem{ref_handsOnMachineLearning}
Aurélien Géron, Hands-on machine learning with Scikit-Learn and TensorFlow concepts, tools, and techniques to build intelligent systems, 2nd ed. O’Reilly Media, Inc., 2019.
‌

\end{thebibliography}

\vfill
\clearpage
\newpage
\appendix

\label{appendix:code1}Code example 1: implementation of the k-means clustering algorythm

\begin{lstlisting}[language=Python]
class KMCModel:
    def __init__(self, file):
        # Load data from CSV file
        self.data = pd.read_csv(file)
        self.movies = self.data['movieId']
        self.tag_relevances = self.data.drop('movieId', axis=1)

        # Create results directory if not exists
        if not os.path.exists("results"):
            os.makedirs("results")

    def plot_clusters(self, column, method_name):
        print(f'running plot_clusters')
        # Reduce dimensionality to 2D using PCA
        pca = PCA(n_components=2)
        pca_result = pca.fit_transform(self.tag_relevances)

        # Create a scatter plot of the two principal components with cluster labels as color
        plt.scatter(pca_result[:, 0], pca_result[:, 1], c=self.data[column], cmap='viridis')
        plt.xlabel('Principal Component 1')
        plt.ylabel('Principal Component 2')
        plt.title(f'{method_name} Clustering')
        plt.savefig(f'results/{method_name}_clustering_2.png')
        plt.clf()
        
    def predict(self,*args,**kwargs):
        return self.kmeansModel.predict(*args,**kwargs)
    
    def getFilmCategory(self, filmId):
        if filmId not in self.movies.values:
            return None
        # return self.data.loc[self.data['movieId']==filmId,'Category'].values[0]
        return self.data.loc[self.data['movieId']==filmId,'Category'].values[0]

    def kmeans_clustering(self, n=10, max_iter=100):
        print(f'running kmeans_clustering')
        # Apply KMeans Clustering
        kmeans = KMeans(n_clusters=n, init='k-means++', max_iter=max_iter, n_init=1)
        kmeans.fit(self.tag_relevances)
        self.data["Category"] = kmeans.predict(self.tag_relevances)
        self.plot_clusters("Category", "KMeans")
        self.kmeansModel = kmeans

    def save_results(self, file):
        print(f'running save_results')
        # Save results to a new CSV file
        self.data.to_csv(file, index=False)

\end{lstlisting}






\end{document}
